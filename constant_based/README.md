# PPO with Fixed Entropy Coefficient â€“ Super Mario Bros

## ðŸ“Œ Overview
This module trains a **Super Mario Bros** agent using **Proximal Policy Optimization (PPO)** with a **constant entropy coefficient** (`ent_coef=0.1`) throughout the entire training process.

It serves as the **baseline** for comparison with other entropy scheduling strategies (progress-based decay, timestep-based decay).

## ðŸš€ Training
Run the training script:
```bash
python mario_learn.py
```
Best model will be saved in:
```bash
./best_model/best_model.zip
```
Training logs and evaluation results will be stored in:
```bash
./logs/
```

## ðŸŽ® Testing a Trained Model
Run:
```bash
python mario_test.py
```
## ðŸ›  Utility Functions
utils.py provides:

* make_env() â€“ Creates the wrapped Mario environment with skip frames, grayscale, and resize.

* make_eval_callback() â€“ Evaluates and saves best model during training.

* set_seed() â€“ Ensures reproducible runs.

* quick_seed_test() â€“ Checks if seeding works as expected.

