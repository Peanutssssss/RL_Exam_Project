# PPO with Timestep-based Entropy Decay â€“ Super Mario Bros

## ðŸ“Œ Overview
This module trains a **Super Mario Bros** agent using **Proximal Policy Optimization (PPO)** with a **timestep-based linear entropy decay**.

Instead of keeping the entropy coefficient constant, it **decreases linearly** as training timesteps increase.  
This method provides a predictable exploration schedule, but may reduce exploration too early in some environments.

## ðŸš€ Training
Run the training script:
```bash
python mario_learn.py
```

The linear decay function:
```bash
Î²(t) = max( Î²_min, Î²_max + (t / T_max) * (Î²_min - Î²_max) )
```
Where:

* t: Current timestep

* T_max: Maximum training timesteps

* Î²_min, Î²_max: Minimum and maximum entropy coefficients

Best model will be saved in:
```bash
./best_model/best_model.zip
```
Training logs and evaluation results will be stored in:
```bash
./logs/
```

## ðŸŽ® Testing a Trained Model
Run:
```bash
python mario_test.py
```

## ðŸ›  Utility Functions
utils.py provides:

* make_env() â€“ Creates the wrapped Mario environment with skip frames, grayscale, and resize.

* make_eval_callback() â€“ Evaluates and saves best model during training.

* set_seed() â€“ Ensures reproducible runs.

* quick_seed_test() â€“ Checks if seeding works as expected.

## ðŸ“Š Expected Behavior
* Early training: Higher entropy â†’ more exploration.

* Later training: Gradual entropy reduction â†’ more exploitation.

* More predictable schedule compared to progress-based decay, but less adaptive to agentâ€™s actual performance.

