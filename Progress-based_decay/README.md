# PPO with Progress-based Entropy Decay â€“ Super Mario Bros

## ðŸ“Œ Overview
This module trains a **Super Mario Bros** agent using **Proximal Policy Optimization (PPO)** with a **progress-based adaptive entropy coefficient**.

Instead of keeping the entropy coefficient constant, it is **dynamically adjusted** based on the agent's **average maximum horizontal position** over recent episodes using a **logistic decay function**.  
This approach aims to balance exploration and exploitation more effectively as the agent progresses in the environment.

## ðŸš€ Training
Run the training script:
```bash
python mario_learn.py
```
**Logistic Decay Function**:

```bash
Î²(p) = Î²_min + (Î²_max - Î²_min) / (1 + exp((p - c) / k))
```

Where:
* p: Average maximum position in recent episodes

* c: Midpoint of decay (controls when the decay is centered)

* k: Smoothness of the decay curve (larger k â†’ slower change)

* Î²_min, Î²_max: Minimum and maximum entropy coefficients

Best model will be saved in:
```bash
./best_model/best_model.zip
```
Training logs and evaluation results will be stored in:
```bash
./logs/
```

## ðŸŽ® Testing a Trained Model
Run:
```bash
python mario_test.py
```
## ðŸ›  Utility Functions
utils.py provides:

* make_env() â€“ Creates the wrapped Mario environment with skip frames, grayscale, and resize.

* make_eval_callback() â€“ Evaluates and saves best model during training.

* set_seed() â€“ Ensures reproducible runs.

* quick_seed_test() â€“ Checks if seeding works as expected.

## ðŸ“Š Expected Behavior
* Early training: Higher entropy â†’ encourages exploration.

* Later training: Lower entropy â†’ focuses on exploitation of learned strategies.

* Typically achieves faster convergence and higher peak performance compared to fixed entropy.

